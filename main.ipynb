{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from survae import SurVAE\n",
    "from survae.calibrate import calc_cs, plot_histogram, plot_cdf\n",
    "from survae.data import ngon, circles, corners, checkerboard, get_spatial_mnist\n",
    "from survae.layer import BijectiveLayer, AbsoluteUnit, OrthonormalLayer, MaxTheLayer, SortingLayer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T23:09:11.168188073Z",
     "start_time": "2024-03-01T23:09:11.156947627Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Shenanigans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_generators = [\n",
    "    lambda: SurVAE(\n",
    "        [\n",
    "            BijectiveLayer(2, [64] * 5),\n",
    "            OrthonormalLayer(2),\n",
    "        ] * 10,\n",
    "    ),\n",
    "    lambda: SurVAE(\n",
    "        [\n",
    "            AbsoluteUnit(torch.tensor([1 / 2, 1 / 2])),\n",
    "        ] + [\n",
    "            BijectiveLayer(2, [64] * 5),\n",
    "            OrthonormalLayer(2),\n",
    "        ] * 10,\n",
    "    ),\n",
    "    lambda: SurVAE(\n",
    "        [\n",
    "            AbsoluteUnit(torch.tensor([1 / 2, 1 / 2]), learn_q=True),\n",
    "        ] + [\n",
    "            BijectiveLayer(2, [64] * 5),\n",
    "            OrthonormalLayer(2),\n",
    "        ] * 10,\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_functions = [ngon, circles, corners, checkerboard]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T23:09:13.072326156Z",
     "start_time": "2024-03-01T23:09:13.059037256Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T23:09:17.716795687Z",
     "start_time": "2024-03-01T23:09:13.069028337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/12] Starting training... done in 4.32 seconds\n",
      "[2/12] Starting training..."
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m model_generator()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(model_generators)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39mj\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(dataset_functions)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(model_generators)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] \u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_count\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m models[i, j] \u001B[38;5;241m=\u001B[39m (model, results)\n",
      "File \u001B[0;32m~/Documents/Education/School/Heidelberg/Studium/Magisterské/3. semestr/Generative Neural Networks for the Sciences (ML)/final/survae/__init__.py:72\u001B[0m, in \u001B[0;36mSurVAE.train\u001B[0;34m(self, sample_function, batch_size, test_size, epochs, lr, log_count)\u001B[0m\n\u001B[1;32m     70\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     71\u001B[0m x_train \u001B[38;5;241m=\u001B[39m sample_function(batch_size)\n\u001B[0;32m---> 72\u001B[0m z, ll \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_log_likelihood\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m loss \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(z \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m-\u001B[39m ll) \u001B[38;5;241m/\u001B[39m batch_size\n\u001B[1;32m     75\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Education/School/Heidelberg/Studium/Magisterské/3. semestr/Generative Neural Networks for the Sciences (ML)/final/survae/__init__.py:37\u001B[0m, in \u001B[0;36mSurVAE.forward\u001B[0;34m(self, X, return_log_likelihood)\u001B[0m\n\u001B[1;32m     35\u001B[0m ll_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 37\u001B[0m     X, ll \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_log_likelihood\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     ll_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m ll\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_log_likelihood:\n",
      "File \u001B[0;32m~/Documents/Education/School/Heidelberg/Studium/Magisterské/3. semestr/Generative Neural Networks for the Sciences (ML)/final/survae/layer.py:196\u001B[0m, in \u001B[0;36mAbsoluteUnit.forward\u001B[0;34m(self, X, return_log_likelihood)\u001B[0m\n\u001B[1;32m    194\u001B[0m Z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mabs\u001B[39m(X)\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_log_likelihood:\n\u001B[0;32m--> 196\u001B[0m     ll \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(log(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq\u001B[49m\u001B[43m[\u001B[49m\u001B[43mX\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m)) \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(log(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq[\u001B[38;5;129;01mnot\u001B[39;00m X \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Z, ll \u001B[38;5;66;03m# TODO: do we have a problem if q is learned to be less than 0? also we should put an upper limit on it\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# Train the datasets\n",
    "models = {}\n",
    "batch_size = 1_000\n",
    "test_size = 10_000\n",
    "epochs = 100\n",
    "log_count = epochs // 10\n",
    "\n",
    "# Iterate over datasets and create heatm aps\n",
    "for i, dataset_function in enumerate(dataset_functions):\n",
    "    for j, model_generator in enumerate(model_generators):\n",
    "        model = model_generator()\n",
    "        print(f\"[{i * len(model_generators) + j + 1}/{len(dataset_functions) * len(model_generators)}] \", end=\"\")\n",
    "        results = model.train(\n",
    "            dataset_function, batch_size=batch_size, test_size=test_size,\n",
    "            epochs=epochs, lr=0.001, log_count=log_count\n",
    "        )\n",
    "\n",
    "        models[i, j] = (model, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-01T23:09:06.315172398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset names\n",
    "model_names = ['NF', 'NF + constant abs', 'NF + trained abs']\n",
    "dataset_names = ['Hex', 'Circles', 'Corners', 'Checkerboard']\n",
    "\n",
    "sample_count = 100_000\n",
    "bins = 100\n",
    "\n",
    "# Create subplots (+1 for the raw data)\n",
    "fig, axs = plt.subplots(len(dataset_names), len(model_names) + 1, figsize=(15, 10))\n",
    "\n",
    "# Plot the raw data\n",
    "for i, (dataset_name, dataset_function) in enumerate(zip(dataset_names, dataset_functions)):\n",
    "    X = dataset_function(sample_count).cpu().numpy()\n",
    "\n",
    "    axs[i, 0].hist2d(X[:, 0], X[:, 1], bins=bins)\n",
    "    axs[i, 0].set_title(f'Data / {dataset_name}')\n",
    "\n",
    "# Iterate over datasets and create heatmaps\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        model = models[i, j][0]  # [0] is the model, [1] is parameters/losses for log epochs\n",
    "        X = model.sample(sample_count).cpu().numpy()\n",
    "\n",
    "        axs[i, j + 1].hist2d(X[:, 0], X[:, 1], bins=bins)\n",
    "        axs[i, j + 1].set_title(f'{model_name} / {dataset_name}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T23:08:23.990942684Z",
     "start_time": "2024-03-01T23:08:23.988752836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create subplots (+1 for the raw data)\n",
    "fig, axs = plt.subplots(len(dataset_names), len(model_names) + 1, figsize=(15, 10))\n",
    "\n",
    "# Plot the raw data\n",
    "for i, (dataset_name, dataset_function) in enumerate(zip(dataset_names, dataset_functions)):\n",
    "    X = dataset_function(sample_count).cpu().numpy()\n",
    "\n",
    "    axs[i, 0].hist2d(X[:, 0], X[:, 1], bins=bins)\n",
    "    axs[i, 0].set_title(f'Data / {dataset_name}')\n",
    "\n",
    "# Iterate over datasets and create heatmaps for losses\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        training_loss = [l for _, l, _ in models[i, j][1].values()]\n",
    "        testing_loss = [l for _, _, l in models[i, j][1].values()]\n",
    "        epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "        axs[i, j + 1].plot(epochs, training_loss, label='Training loss')\n",
    "        axs[i, j + 1].plot(epochs, testing_loss, label='Testing loss')\n",
    "        axs[i, j + 1].set_title(f'{model_name} / {dataset_name}')\n",
    "        axs[i, j + 1].set_xlabel('Epoch')\n",
    "        axs[i, j + 1].set_ylabel('Loss')\n",
    "        axs[i, j + 1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-01T23:08:24.030134314Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: go through the models and plot the values\n",
    "fig, axs = plt.subplots(len(dataset_names), 1, figsize=(15, 10))\n",
    "\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    # We're interested in the last model - that is the one that changes the q\n",
    "    model, states = models[i, len(model_names) - 1]\n",
    "\n",
    "    qs = []\n",
    "    for epoch, (state_dict, _, _) in states.items():\n",
    "        model.load_state_dict(state_dict)\n",
    "        qs.append(model.layers[0].q.item())\n",
    "\n",
    "    epochs = range(1, len(qs) + 1)\n",
    "\n",
    "    axs[i].plot(epochs, qs)\n",
    "    axs[i].set_title(f'Q-values for {model_names[-1]} / {dataset_name}')\n",
    "    axs[i].set_xlabel('Epoch')\n",
    "    axs[i].set_ylabel('Q-value')\n",
    "    axs[i].set_ylim([0, 1])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: take me from the training before\n",
    "model = SurVAE(\n",
    "    [\n",
    "        AbsoluteUnit(torch.tensor([1 / 2, 1 / 2]), learn_q=True),\n",
    "    ] + [\n",
    "        BijectiveLayer(2, [64] * 5),\n",
    "        OrthonormalLayer(2),\n",
    "    ] * 10,\n",
    ")\n",
    "\n",
    "model.train(ngon, batch_size=1000, epochs=100, lr=0.001);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-01T13:46:01.154601800Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test = model.sample(100)\n",
    "cs = calc_cs(model, X_test, 50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-01T13:46:01.158128458Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T13:46:01.169467184Z",
     "start_time": "2024-03-01T13:46:01.160843190Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(cs[0], 20, \"Calibration diagram for x-axis\")\n",
    "plot_histogram(cs[1], 20, \"Calibration diagram for y-axis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-01T13:46:01.172931840Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_cdf(cs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T14:20:57.439336928Z",
     "start_time": "2024-03-01T14:20:57.396184350Z"
    }
   },
   "outputs": [],
   "source": [
    "model_generators = [\n",
    "    lambda: SurVAE(\n",
    "        [\n",
    "            BijectiveLayer(100, [64] * 5),\n",
    "            OrthonormalLayer(100),\n",
    "        ] * 100,\n",
    "    ),\n",
    "    lambda: SurVAE(\n",
    "        [\n",
    "            BijectiveLayer(100, [64] * 5),\n",
    "            OrthonormalLayer(100),\n",
    "        ] * 50 + [\n",
    "            SortingLayer(),\n",
    "        ] + [\n",
    "            BijectiveLayer(100, [64] * 5),\n",
    "            OrthonormalLayer(100),\n",
    "        ] * 49,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Starting training..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m model_generator()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(model_generators)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] \u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mget_spatial_mnist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_count\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m models[i] \u001B[38;5;241m=\u001B[39m (model, results)\n",
      "File \u001B[0;32m~/Documents/Education/School/Heidelberg/Studium/Magisterské/3. semestr/Generative Neural Networks for the Sciences (ML)/final/survae/__init__.py:72\u001B[0m, in \u001B[0;36mSurVAE.train\u001B[0;34m(self, sample_function, batch_size, test_size, epochs, lr, log_count)\u001B[0m\n\u001B[1;32m     70\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     71\u001B[0m x_train \u001B[38;5;241m=\u001B[39m sample_function(batch_size)\n\u001B[0;32m---> 72\u001B[0m z, ll \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_log_likelihood\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m loss \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(z \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m-\u001B[39m ll) \u001B[38;5;241m/\u001B[39m batch_size\n\u001B[1;32m     75\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Education/School/Heidelberg/Studium/Magisterské/3. semestr/Generative Neural Networks for the Sciences (ML)/final/survae/__init__.py:37\u001B[0m, in \u001B[0;36mSurVAE.forward\u001B[0;34m(self, X, return_log_likelihood)\u001B[0m\n\u001B[1;32m     35\u001B[0m ll_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 37\u001B[0m     X, ll \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_log_likelihood\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     ll_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m ll\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_log_likelihood:\n",
      "File \u001B[0;32m~/Documents/Education/School/Heidelberg/Studium/Magisterské/3. semestr/Generative Neural Networks for the Sciences (ML)/final/survae/layer.py:129\u001B[0m, in \u001B[0;36mBijectiveLayer.forward\u001B[0;34m(self, X, return_log_likelihood)\u001B[0m\n\u001B[1;32m    126\u001B[0m non_skip_connection \u001B[38;5;241m=\u001B[39m X[:, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskip_size:]\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# compute coefficients for linear transformation\u001B[39;00m\n\u001B[0;32m--> 129\u001B[0m coeffs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mffnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mskip_connection\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;66;03m# split output into t and pre_s\u001B[39;00m\n\u001B[1;32m    131\u001B[0m t \u001B[38;5;241m=\u001B[39m coeffs[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Education/School/Heidelberg/Studium/Magisterské/3. semestr/Generative Neural Networks for the Sciences (ML)/final/survae/layer.py:34\u001B[0m, in \u001B[0;36mFFNN.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m---> 34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1601\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1598\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_backward_pre_hooks\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m:\n\u001B[1;32m   1599\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;241m=\u001B[39m OrderedDict()\n\u001B[0;32m-> 1601\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tensor, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModule\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m:\n\u001B[1;32m   1603\u001B[0m         _parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the datasets\n",
    "models = {}\n",
    "batch_size = 1_000\n",
    "test_size = 10_000\n",
    "epochs = 1_000\n",
    "log_count = epochs // 10\n",
    "\n",
    "# Iterate over datasets and create heatm aps\n",
    "for i, model_generator in enumerate(model_generators):\n",
    "    model = model_generator()\n",
    "    print(f\"[{i + 1}/{len(model_generators)}] \", end=\"\")\n",
    "\n",
    "    results = model.train(\n",
    "        get_spatial_mnist, batch_size=batch_size, test_size=test_size,\n",
    "        epochs=epochs, lr=0.001, log_count=log_count\n",
    "    )\n",
    "\n",
    "    models[i] = (model, results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T14:34:24.555731445Z",
     "start_time": "2024-03-01T14:20:57.967946633Z"
    }
   },
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_names = ['NF', 'NF + sorting']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-01T14:34:24.550385090Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rows = 4 * (1 + len(model_generators))\n",
    "columns = 10\n",
    "smnist = get_spatial_mnist(rows * columns)\n",
    "\n",
    "# Plot the first k*l images with sampled points in a 2D grid\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=columns, figsize=(15, 10))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        if i // 4 == 0:\n",
    "            sample = smnist[i * columns + j]\n",
    "        elif i // 4 == 1:\n",
    "            sample = models[0][0].sample(1)[0]\n",
    "        elif i // 4 == 2:\n",
    "            sample = models[1][0].sample(1)[0]\n",
    "\n",
    "        ax = axes[i, j]\n",
    "        points = sample.cpu().numpy().reshape(50, 2)\n",
    "\n",
    "        ax.scatter(points[:, 0], -points[:, 1], s=15, label='Sampled Points')\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-01T14:34:24.551725444Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create subplots (+1 for the raw data)\n",
    "fig, axs = plt.subplots(1, len(model_names), figsize=(15, 10))\n",
    "\n",
    "# Iterate over datasets and create heatmaps for losses\n",
    "for i, model_name in enumerate(model_names):\n",
    "    training_loss = [l for _, l, _ in models[i][1].values()]\n",
    "    testing_loss = [l for _, _, l in models[i][1].values()]\n",
    "    epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "    axs[i].plot(epochs, training_loss, label='Training loss')\n",
    "    axs[i].plot(epochs, testing_loss, label='Testing loss')\n",
    "    axs[i].set_title(f'{model_name}')\n",
    "    axs[i].set_xlabel('Epoch')\n",
    "    axs[i].set_ylabel('Loss')\n",
    "    axs[i].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-01T14:34:24.552035635Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
