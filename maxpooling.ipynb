{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from survae import SurVAE, DEVICE\n",
    "from survae.data import MNIST_784\n",
    "from survae.layer import *\n",
    "from survae.calibrate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = MNIST_784()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_maxpool = SurVAE(\n",
    "    # [DequantizationLayer()] + # the dequantization layer makes the reconstruction error pretty pointless in practice\n",
    "    [\n",
    "        [BijectiveLayer(784, [200, 200]), OrthonormalLayer(784)] # 784 = 28^2\n",
    "        for _ in range(5)\n",
    "    ] +\n",
    "    [MaxPoolingLayer(784, 2, learn_distribution_parameter=True)] + \n",
    "    [\n",
    "        [BijectiveLayer(196, [200, 200]), OrthonormalLayer(196)] # 196 = 14^2\n",
    "        for _ in range(5)\n",
    "    ] +\n",
    "    [MaxPoolingLayer(196, 2, learn_distribution_parameter=True)] +\n",
    "    [\n",
    "        [BijectiveLayer(49, [200, 200]), OrthonormalLayer(49)] # 49 = 7^2\n",
    "        for _ in range(5)\n",
    "    ] +\n",
    "    [MaxPoolingLayerWithHop(49, 3, 2, learn_distribution_parameter=True)] +\n",
    "    [\n",
    "        [BijectiveLayer(9, [200, 200]), OrthonormalLayer(9)] # 9 = 3^2\n",
    "        for _ in range(5)\n",
    "    ],\n",
    "    name = \"SV_MAXPOOL\",\n",
    "    condition_size = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = sv_maxpool.train(\n",
    "    dataset    = mnist_dataset,\n",
    "    batch_size = 1000,\n",
    "    test_size  = 100,\n",
    "    epochs     = 40_000,\n",
    "    lr         = 5e-3,\n",
    "    log_period = 100,\n",
    "    show_tqdm  = True,\n",
    "    reconstruction_loss_weight = 0.01,\n",
    "    lr_decay_params = {'gamma': 0.95, 'step_size': 500},\n",
    "    save_path = './saves/sv_maxpool overnight',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = list(log.keys())\n",
    "loss_train = [m.training_loss for m in log.values()]\n",
    "loss_test = [m.testing_loss for m in log.values()]\n",
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.plot(times, loss_train, label='Training loss')\n",
    "plt.plot(times, loss_test, label='Validation loss')\n",
    "\n",
    "plt.title('Loss of MNIST network during training')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('NLL Loss')\n",
    "\n",
    "plt.grid()\n",
    "# plt.ylim(0, 5000)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that sigma parameters are actually being learned\n",
    "for _layer in sv_maxpool.layers:\n",
    "    if isinstance(_layer, (MaxPoolingLayer, MaxPoolingLayerWithHop)):\n",
    "        print(f\"{(_s := _layer.sigma).item():.4f}\", _s.requires_grad, f\"{_s.grad.item():8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learned_distribution(\n",
    "        Y: torch.Tensor,\n",
    "        title: str,\n",
    "        plotsize: float = 2,\n",
    "        axis_scale: float = 3.6,\n",
    "        bins: int = 40,\n",
    "        alpha: float = 0.15,\n",
    "        sigma: float = 1.0,\n",
    "):\n",
    "    '''\n",
    "    Check if a distribution looks gaussian in 1D and 2D.\n",
    "    A circle is drawn in each scatter plot as a visual cue to more easily compare the spreads.\n",
    "    The radius is chosen such that 90% of the samples of a gaussian distribution are expected\n",
    "    to lie inside the circle.\n",
    "\n",
    "    ### Inputs:\n",
    "    * Y: Output of distribution.\n",
    "    * plotsize: Size of each subplot.\n",
    "    * axis_scale: \"Zoom factor\" of the plots.\n",
    "    * bins: Number of bins in the 1D plots.\n",
    "    * alpha: Transparency of the scattered points.\n",
    "    * sigma: Standard deviation of the normal distribution to compare with.\n",
    "    '''\n",
    "    n = Y.shape[-1] # number of parameters\n",
    "\n",
    "    fig, ax = plt.subplots(n, n, figsize=(plotsize * n, plotsize * n))\n",
    "\n",
    "    x_axis = np.linspace(-axis_scale, axis_scale, 100)\n",
    "    gaussian_pdf = lambda x: 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-0.5 * (x / sigma) ** 2)\n",
    "    y_gaussian = gaussian_pdf(x_axis)\n",
    "    circle_radius = sigma * np.sqrt(-2 * np.log(0.1))\n",
    "\n",
    "    for i in range(n):\n",
    "        for k in range(n):\n",
    "            _ax = ax[i][k]\n",
    "\n",
    "            _ax.set_xticks([])\n",
    "            _ax.set_yticks([])\n",
    "\n",
    "            if i == k:\n",
    "                _ax.hist(Y[:, i], bins=bins, density=True, range=(-axis_scale, axis_scale))\n",
    "                _ax.plot(x_axis, y_gaussian, color='r')\n",
    "\n",
    "                _ax.set_xlim(-axis_scale, axis_scale)\n",
    "            else:\n",
    "                # the y axis is flipped so that the graphs are mirrored on the diagonal\n",
    "                # (e.g. subplot (1, 2) is a mirror image of subplot (2, 1))\n",
    "                _ax.scatter(Y[:, i], -Y[:, k], s=1, alpha=alpha)\n",
    "                _ax.plot(0, 0, 'ro', markersize=2)\n",
    "                _ax.add_patch(plt.Circle((0, 0), radius=circle_radius, color='r', fill=False))\n",
    "\n",
    "                _ax.set_xlim(-axis_scale, axis_scale)\n",
    "                _ax.set_ylim(-axis_scale, axis_scale)\n",
    "                _ax.set_aspect('equal')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist_dataset.sample(1_000, labels=True)\n",
    "y = mnist_dataset.label_to_one_hot(y, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z = sv_maxpool(X, y).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learned_distribution(Z, \"Learned distribution of MNIST model\", plotsize=1.4, alpha=0.5, sigma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 5\n",
    "nrows = 4\n",
    "plotsize = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1's\n",
    "_y = torch.tensor([1]).expand(ncols * nrows)\n",
    "y = MNIST_784.label_to_one_hot(_y, 10)\n",
    "X_hat = sv_maxpool.sample(len(y), y).view(nrows, ncols, 28, 28).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually specify the standard deviation for the samples\n",
    "_sigma = 0.5\n",
    "\n",
    "Z_hat = torch.normal(0, _sigma, size=(ncols * nrows, 9), device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    X_hat = sv_maxpool.backward(Z_hat, y).view(nrows, ncols, 28, 28).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows, ncols, figsize=(plotsize * ncols, plotsize * nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        _ax = ax[i, j]\n",
    "        _ax.imshow(X_hat[i, j])\n",
    "        _ax.set_xticks([])\n",
    "        _ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxPoolingLayer development (kept for potential use in report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 3\n",
    "k = torch.distributions.categorical.Categorical(torch.tensor([1/stride**2] * stride**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((5, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_x = k.sample((5, 2, 2))\n",
    "i_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = i_x.repeat_interleave(3, dim=2).repeat_interleave(3, dim=1)\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.arange(stride**2).reshape(stride, stride).repeat(2, 2)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (m == j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = torch.distributions.half_normal.HalfNormal(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = distr.sample((5, 6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise[mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.repeat_interleave(stride, dim=1).repeat_interleave(stride, dim=2)# - noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
