{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from survae import SurVAE\n",
    "from survae.data import Dataset\n",
    "from survae.layer import BijectiveLayer, AbsoluteUnit, OrthonormalLayer, SortingLayer, MaxPoolingLayer, MaxTheLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import exp, tanh, log\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Layer(nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    Abstract class used as the framework for all types of layers used in the SurVAE-Flow architecture.\n",
    "\n",
    "    All layers are defined in the inference direction, i.e. the 'forward' method sends elements of the\n",
    "    data space X to the latent space Z, whereas the 'backward' method goes from Z to X.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X: torch.Tensor, condition: torch.Tensor | None = None, return_log_likelihood: bool = False):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the layer and, optionally, the log likelihood contribution as a scalar (i.e. already summed).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, Z: torch.Tensor, condition: torch.Tensor | None = None):\n",
    "        \"\"\"\n",
    "        Computes the backward pass of the layer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def in_size(self) -> int | None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def out_size(self) -> int | None:\n",
    "        pass\n",
    "\n",
    "    def make_conditional(self, size: int):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MaxPoolingLayer(Layer):\n",
    "    # TODO now: implement ohne Ãœberlappungen\n",
    "    def __init__(self, size: int, stride: int, lam = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = np.sqrt(size).astype(int) \n",
    "\n",
    "        assert self.size % stride == 0, \"Stride must be a divisor of size!\"\n",
    "        self.stride = stride\n",
    "\n",
    "        self.lam = lam\n",
    "\n",
    "        self.index_probs = torch.tensor([1 / self.stride for _ in range(self.stride)])\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.Tensor, condition: torch.Tensor | None = None, return_log_likelihood: bool = False):\n",
    "\n",
    "        X = X.view(self.size, self.size) # reshape to 2D\n",
    "        \n",
    "        l = []\n",
    "        for i in range(self.stride):\n",
    "            for j in range(self.stride):\n",
    "                l.append(X[i::self.stride,j::self.stride])\n",
    "\n",
    "        combined_tensor = torch.stack(l, dim=0)\n",
    "        Z, _ = torch.max(combined_tensor, dim=0)\n",
    "        return Z.view(-1)\n",
    "\n",
    "    def backward(self, Z: torch.Tensor, condition: torch.Tensor | None = None):\n",
    "        exp_distr = torch.distributions.exponential.Exponential(self.lam)\n",
    "        Z = Z.view((self.out_size(), self.out_size()))\n",
    "\n",
    "        # expand matrix containing local maxima \n",
    "        X_hat = Z.repeat_interleave(self.stride,dim=0).repeat_interleave(self.stride,dim=1)\n",
    "\n",
    "        # sample values in (- infty, 0]) with exponential distribution\n",
    "        exp_distr = torch.distributions.exponential.Exponential(self.lam)\n",
    "        samples = -exp_distr.sample(X_hat.shape)\n",
    "\n",
    "\n",
    "        # mask for the indices of the local maxima\n",
    "\n",
    "        k = torch.distributions.categorical.Categorical(self.index_probs) \n",
    "        i_indices = k.sample((self.out_size()**2,))\n",
    "        j_indices = k.sample((self.out_size()**2,))\n",
    "\n",
    "        index_mask = torch.ones_like(X_hat)\n",
    "\n",
    "        for I in range(self.out_size()):\n",
    "            for J in range(self.out_size()):\n",
    "                index_mask[I*self.stride + i_indices[I*self.out_size()+J], J*self.stride + j_indices[I*self.out_size()+J]] = 0\n",
    "\n",
    "        X_hat = X_hat + samples * index_mask\n",
    "        \n",
    "        return X_hat.view(-1)\n",
    "\n",
    "    def in_size(self) -> int | None:\n",
    "        return self.size\n",
    "\n",
    "    def out_size(self) -> int | None:\n",
    "        return int(self.size / self.stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenedsize = 28*28\n",
    "in_size = 28\n",
    "size = np.sqrt(flattenedsize).astype(int)\n",
    "stride = 4\n",
    "out_size = int((in_size / stride))\n",
    "\n",
    "\n",
    "MP = MaxPoolingLayer(flattenedsize, stride)\n",
    "X = torch.arange(flattenedsize)\n",
    "\n",
    "Z = MP.forward(X)\n",
    "\n",
    "X_hat = MP.backward(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
